{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation d'image avec 12 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import keras.models as models\n",
    "#import keras.layers.containers as containers\n",
    "from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape,  Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "#os.environ['KERAS_BACKEND'] = 'theano'\n",
    "# os.environ['KERAS_BACKEND']=\"tensorflow\"\n",
    "# os.environ['THEANO_FLAGS']='mode=FAST_RUN,device=gpu0,floatX=float32,optimizer=fast_compile'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"CamVid\")\n",
    "os.chdir(\"train\")\n",
    "train_images=os.listdir()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"trainannot\")\n",
    "train_targets = os.listdir()\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarylab(labels):\n",
    "    x = np.zeros([360,480,12])    \n",
    "    for i in range(360):\n",
    "        for j in range(480):\n",
    "            x[i,j,labels[i][j]]=1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "Y_train=[]\n",
    "for path_image in train_images[:50]:\n",
    "    img=plt.imread(\"./CamVid/train\"+\"/\"+path_image)\n",
    "    img = (img[:,:,0]+img[:,:,1]+img[:,:,2])/3\n",
    "    img = np.expand_dims(img,axis=2)\n",
    "    X_train.append(img)\n",
    "    img=cv2.imread(\"./CamVid/trainannot\"+\"/\"+path_image)\n",
    "#     for i in range(img.shape[0]):\n",
    "#         for j in range(img.shape[1]):\n",
    "#             if \n",
    "    Y_train.append(np.expand_dims(np.concatenate(img[:,:,0]),axis=1))\n",
    "    \n",
    "#     img=cv2.resize(img,(224,224))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import pylab as pl\n",
    "import matplotlib.cm as cm\n",
    "import itertools\n",
    "import numpy as np\n",
    "#import theano as T\n",
    "\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "#import theano.tensor as T\n",
    "np.random.seed(1337) # for reproducibility\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "#import keras.layers.containers as containers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "import keras.models as models\n",
    "#import keras.layers.containers as containers\n",
    "from keras.layers.core import Layer, Dense, Dropout, Activation, Flatten, Reshape,  Permute\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "path = './CamVid/'\n",
    "data_shape = 360*480"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching \n",
    "I equalized histograms of rbg layers separately, so that change in lighting doesnt effect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367\n",
      ".................................................."
     ]
    }
   ],
   "source": [
    "def normalized(rgb):\n",
    "    #return rgb/255.0\n",
    "    norm=np.zeros((rgb.shape[0], rgb.shape[1], 3),np.float32)\n",
    "\n",
    "    b=rgb[:,:,0]\n",
    "    g=rgb[:,:,1]\n",
    "    r=rgb[:,:,2]\n",
    "\n",
    "    norm[:,:,0]=cv2.equalizeHist(b)\n",
    "    norm[:,:,1]=cv2.equalizeHist(g)\n",
    "    norm[:,:,2]=cv2.equalizeHist(r)\n",
    "\n",
    "    return norm\n",
    "\n",
    "def binarylab(labels):\n",
    "    x = np.zeros([360,480,12])    \n",
    "    for i in range(360):\n",
    "        for j in range(480):\n",
    "            x[i,j,labels[i][j]]=1\n",
    "    return x\n",
    "\n",
    "def prep_data():\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    import os\n",
    "    with open(path+'train.txt') as f:\n",
    "        txt = f.readlines()\n",
    "        txt = [line.split(' ') for line in txt]\n",
    "    print(len(txt))\n",
    "    for i in range(367):#len(txt)\n",
    "        ## these paths are very specific to my machine\n",
    "        train_data.append(normalized(cv2.imread(os.getcwd() + txt[i][0][7:])))#np.rollaxis(,2)\n",
    "        train_label.append(binarylab(cv2.imread(os.getcwd() + txt[i][1][7:][:-1])[:,:,0]))\n",
    "        print('.',end='')\n",
    "    return np.array(train_data), np.array(train_label)\n",
    "\n",
    "train_data, train_label = prep_data()\n",
    "\n",
    "train_label = np.reshape(train_label,(367,data_shape,12))#367\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Compiling neuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 360, 480, 12)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(Y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"valid\")`\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:25: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:31: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:62: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:68: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), padding=\"valid\")`\n",
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(12, (1, 1), padding=\"valid\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_26 (Sequential)   (None, 45, 60, 512)       1553664   \n",
      "_________________________________________________________________\n",
      "sequential_27 (Sequential)   (None, 360, 480, 64)      3912384   \n",
      "_________________________________________________________________\n",
      "sequential_28 (Sequential)   (None, 172800, 12)        780       \n",
      "=================================================================\n",
      "Total params: 5,466,828\n",
      "Trainable params: 5,462,988\n",
      "Non-trainable params: 3,840\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Thomas\\Anaconda3\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:95: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "data_shape=360*480\n",
    "def encoder():\n",
    "        kernel = 3\n",
    "        filter_size = 64\n",
    "        pad = 1\n",
    "        pool_size = 2\n",
    "    \n",
    "        E = Sequential()\n",
    "        \n",
    "        E.add(Layer(input_shape=(360, 480,1)))\n",
    "        E.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        \n",
    "        E.add(Convolution2D(filter_size, (kernel, kernel), border_mode='valid'))\n",
    "        E.add(BatchNormalization())\n",
    "        E.add(Activation('relu'))\n",
    "        E.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "        \n",
    "        E.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        E.add(Convolution2D(128, kernel, kernel, border_mode='valid'))\n",
    "        E.add(BatchNormalization())\n",
    "        E.add(Activation('relu'))\n",
    "        E.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "        \n",
    "        E.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        E.add(Convolution2D(256, kernel, kernel, border_mode='valid'))\n",
    "        E.add(BatchNormalization())\n",
    "        E.add(Activation('relu'))\n",
    "        E.add(MaxPooling2D(pool_size=(pool_size, pool_size)))\n",
    "\n",
    "        E.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        E.add(Convolution2D(512, kernel, kernel, border_mode='valid'))\n",
    "        E.add(BatchNormalization())\n",
    "        \n",
    "        E.add(Activation('relu'))\n",
    "        #E.summary()\n",
    "\n",
    "        return E\n",
    "    \n",
    "def decoder():\n",
    "    \n",
    "        kernel = 3\n",
    "        filter_size = 64\n",
    "        pad = 1\n",
    "        pool_size = 2\n",
    "        \n",
    "        D = Sequential()\n",
    "        \n",
    "        D.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        D.add(Convolution2D(512, kernel, kernel, border_mode='valid'))\n",
    "        D.add(BatchNormalization())\n",
    "\n",
    "        #D.add(UnPooling2D(poolsize=(pool_size,pool_size)))\n",
    "        D.add(UpSampling2D())\n",
    "        \n",
    "        D.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        D.add(Convolution2D(256, kernel, kernel, border_mode='valid'))\n",
    "        D.add(BatchNormalization())\n",
    "\n",
    "        #D.add(UnPooling2D(poolsize=(pool_size,pool_size)))\n",
    "        D.add(UpSampling2D())\n",
    "        D.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        D.add(Convolution2D(128, kernel, kernel, border_mode='valid'))\n",
    "        D.add(BatchNormalization())\n",
    "\n",
    "        #D.add(UnPooling2D(poolsize=(pool_size,pool_size)))\n",
    "        D.add(UpSampling2D())\n",
    "        D.add(ZeroPadding2D(padding=(pad,pad)))\n",
    "        D.add(Convolution2D(filter_size, kernel, kernel, border_mode='valid'))\n",
    "        D.add(BatchNormalization())\n",
    "        #D.summary()\n",
    "    \n",
    "        return D\n",
    "    \n",
    "def reshapeData() :\n",
    "    dataReshape = Sequential()\n",
    "    dataReshape.add(Convolution2D(12, 1, 1, border_mode='valid',))\n",
    "    dataReshape.add(Reshape((12,data_shape)))\n",
    "    dataReshape.add(Permute((2, 1)))\n",
    "    dataReshape.add(Activation('softmax'))\n",
    "    return dataReshape\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(encoder())\n",
    "model.add(decoder())\n",
    "model.add(reshapeData())\n",
    "model.summary()\n",
    "#model.compile(loss=\"categorical_crossentropy\", optimizer='adadelta')\n",
    "model.compile(loss='sparse_categorical_crossentropy', # using the cross-entropy loss function\n",
    "              optimizer='adam', # using the Adam optimiser\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 5\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(np.array(X_train), np.array(Y_train), batch_size=batch_size, nb_epoch=nb_epoch, verbose=1 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot((history.history['loss']))\n",
    "plt.plot((history.history['val_loss']))\n",
    "plt.title('Model loss by epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot((history.history['loss']))\n",
    "plt.plot((history.history['val_loss']))\n",
    "plt.title('Model loss from 500 to 2000')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'valid'], loc='right')\n",
    "plt.xlim((500, 2000))\n",
    "plt.ylim((50000, 160000))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
